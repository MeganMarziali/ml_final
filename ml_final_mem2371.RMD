---
title: "Machine Learning Final"
author: "Megan Marziali"
date: "3/25/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(caret)
library(rpart.plot)
library(Amelia)
library(arsenal)
library(factoextra)
library(cluster)

set.seed(100)
```

# Part 1: Dietary patterns using unsupervised analysis

## Question 1: Construct a research question

The goal of this unsupervised analysis will be to develop classes of dietary patterns among women during pregnancy. These classes will be used as an exposure in future analyses, aiming to predict whether dietary patterns during pregnancy are predictive of postpartum mental health systems (such as post-partum depression). This is a predictive research question.

## Question 2: Running appropriate unsupervised analysis

```{r}
diet.data = 
  read.csv("./data/diet_data.csv", header = TRUE) %>% 
  select(
    "h_cereal_preg_Ter",
    "h_dairy_preg_Ter",
    "h_fastfood_preg_Ter",
    "h_fish_preg_Ter",
    "h_fruit_preg_Ter",
    "h_legume_preg_Ter",
    "h_meat_preg_Ter",
    "h_veg_preg_Ter",
  )

# Scaling data
colMeans(diet.data, na.rm = TRUE)
apply(diet.data, 2, sd, na.rm = TRUE)
```

I restricted the data to variables which assessed fdietary patterns during pregnancy. The total dataset is comprised of 1301 observations, and 8 variables.

```{r}
missmap(diet.data, main = "Missing values vs observed")
```

I used missmap to determine if there are any missing observations, which there are not. To determine clusters within the data, a hierarchical cluster analysis with the complete linkage method will be carried out.

```{r}
set.seed(100)

# Create Dissimilarity matrix
diss.matrix = dist(diet.data, method = "euclidean")

#Identifying the optimal number of clusters given complete linkage method
gap_stat_c = clusGap(diet.data, FUN = hcut, hc_method = "complete", K.max = 10, B = 50)
fviz_gap_stat(gap_stat_c)

#Characterizing the clusters
clusters.c = hcut(diet.data, k = 2, hc_func = "hclust", hc_method = "complete", hc_metric = "euclidian")

clusters.c$size
fviz_dend(clusters.c, rect = TRUE)
fviz_cluster(clusters.c)
```

The optimal number of clusters within this data is *2* clusters. There are 840 observations in the first cluster, and 461 observations in the second cluster.

```{r}
input.feature.vals = cbind(diet.data, cluster = clusters.c$cluster)
input.feature.vals %>%
 group_by(cluster) %>%
 summarise_all(mean) %>% 
  knitr::kable()
```

The first cluster has less cereal consumed during pregnancy by mothers than the second cluster; respondents also consumed slightly more dairy, fast food, and fish. Those in the second cluster consumed more cereal and slightly more fruit and meats. Both clusters consumed approximately the same amount of vegetables.

# Part 2: Choose your own supervised adventure

## Research question

The goal of this analysis is to generate hypotheses regarding social capital and toxic metals during pregnancy on child externalizing and internalizing behaviors. This hypotheses generating analysis will guide future research on the role of social capital on child behaviors, and whether interactions between social capital and toxic metals are salient for child behaviors. This is being done to generate hypotheses regarding the importance of both the built environment and the social environment on child well-being, and to gain a better understanding of how these factors interact in relation to child health outcomes.

## Loading and preparing data

Loading data into single data frame.

```{r dataprep, message = FALSE, warning = FALSE}
#Load data using path of where file is stored
load("./data/exposome.RData")

#Merge all data frames into a single data frame. FYI, this is just a shortcut by combining baseR with piping from tidyverse. There are other ways of merging across three data frames that are likely more elegant.

studydata = 
  merge(exposome,phenotype,by = "ID") %>% 
  merge(covariates, by = "ID")

#Strip off ID Variable
studydata$ID = NULL
```

Data cleaning and selecting relevant features to research question.

```{r message = FALSE, warning = FALSE}
studydata  = studydata %>% 
  select(hs_as_m_Log2,
         hs_cd_m_Log2,
         hs_co_m_Log2,
         hs_cs_m_Log2,
         hs_cu_m_Log2,
         hs_hg_m_Log2,
         hs_mn_m_Log2,
         hs_mo_m_Log2,
         hs_pb_m_Log2,
         hs_tl_mdich_None,
         hs_dde_madj_Log2,
         hs_ddt_madj_Log2,
         hs_hcb_madj_Log2,
         hs_pcb118_madj_Log2,
         hs_pcb138_madj_Log2,
         hs_pcb153_madj_Log2,
         hs_pcb170_madj_Log2,
         hs_pcb180_madj_Log2,
         hs_sumPCBs5_madj_Log2,
         hs_dep_madj_Log2,
         hs_detp_madj_Log2,
         hs_dmp_madj_Log2,
         hs_dmtp_madj_Log2,
         hs_pbde153_madj_Log2,
         hs_pbde47_madj_Log2,
         hs_pfhxs_m_Log2,
         hs_pfna_m_Log2,
         hs_pfoa_m_Log2,
         hs_pfos_m_Log2,
         hs_pfunda_m_Log2,
         hs_bpa_madj_Log2,
         hs_bupa_madj_Log2,
         hs_etpa_madj_Log2,
         hs_mepa_madj_Log2,
         hs_oxbe_madj_Log2,
         hs_prpa_madj_Log2,
         hs_trcs_madj_Log2,
         hs_mbzp_madj_Log2,
         hs_mecpp_madj_Log2,
         hs_mehhp_madj_Log2,
         hs_mehp_madj_Log2,
         hs_meohp_madj_Log2,
         hs_mep_madj_Log2,
         hs_mibp_madj_Log2,
         hs_mnbp_madj_Log2,
         hs_ohminp_madj_Log2,
         hs_oxominp_madj_Log2,
         hs_sumDEHP_madj_Log2,
         hs_cotinine_mcat_None,
         FAS_cat_None,
         hs_contactfam_3cat_num_None,
         hs_hm_pers_None,
         hs_participation_3cat_None,
         hs_Gen_Tot
         ) 

data.rec = studydata %>% 
  mutate(
    behav = cut(hs_Gen_Tot, 
                breaks = c(-Inf, 60, Inf),
                labels = c("0", "1"))
  ) %>% 
  select(-hs_Gen_Tot) %>% 
  select(everything(), behav)
```

Data exploration.

```{r message = FALSE, warning = FALSE, results = "asis"}
# Investigating missing data
missmap(studydata)

# No missingness observed.

# Exploring continuous/categorical variables of interest
table.1 = tableby(~ behav + FAS_cat_None + hs_contactfam_3cat_num_None +
                    hs_sumDEHP_madj_Log2 + hs_ohminp_madj_Log2 + hs_mibp_madj_Log2 + hs_mehp_madj_Log2,
                  data = data.rec,
        numeric.stats = c("mean","median", "range"))
summary(table.1, text = TRUE)
```

The data is unbalanced, which will be taken into account by down-sampling.

Partitioning data.

```{r message = FALSE, warning = FALSE}
set.seed(100)

#Partition data for use in demonstration
train.indices = createDataPartition(y = data.rec$behav, p = 0.7,list = FALSE)
training = data.rec[train.indices, ]
testing = data.rec[-train.indices, ]
```

## First Algorithm: Using LASSO for Feature Selection

Hyperparameter tuning and initial accuracy testing.

```{r, warning = FALSE, message = FALSE}
set.seed(100)

#Create grid to search lambda
lambda = 10^seq(-3,3, length = 100)

lasso.m1 = train(
  behav ~., 
  data = training, 
  method = "glmnet", 
  trControl = trainControl("cv", number = 10, sampling = "down"), 
  tuneGrid = expand.grid(alpha = 0, lambda = lambda)
)

#Print the values of alpha and lambda that gave best prediction
lasso.m1$bestTune

#Print all of the options examined
lasso.m1$results

# Model coefficients
coef(lasso.m1$finalModel, lasso.m1$bestTune$lambda)
varImp(lasso.m1)

# Make predictions
pred.lasso.1 = predict(lasso.m1, training)
pred.lasso.prob.1 = predict(lasso.m1, training, type = "prob")

# Model prediction performance
eval.results = confusionMatrix(pred.lasso.1, training$behav, positive = "1")
print(eval.results)

#Accuracy of this model is 0.65
```

## Second Algorithm: Elastic Net

```{r}
set.seed(100)

alpha = seq(0,1, length = 100)

en.m = train(
  behav ~., 
  data = training, 
  method = "glmnet", 
  trControl = trainControl("cv", number = 10, sampling = "down"), 
  tuneGrid = expand.grid(alpha = alpha, lambda = lambda)
)

#Print the values of alpha and lambda that gave best prediction
en.m$bestTune

#Print all of the options examined
en.m$results

# Model coefficients
coef(en.m$finalModel, en.m$bestTune$lambda)
varImp(en.m)

# Make predictions
pred.en = predict(en.m, training)
pred.en.prob = predict(en.m, training, type = "prob")

# Model prediction performance
eval.results = confusionMatrix(pred.en, training$behav, positive = "1")
print(eval.results)

#Accuracy of this model is 0.58
```

Given that the LASSO model produced slightly better accuracy than the elastic net model, the LASSO model will be used for final accuracy testing.

### Final accuracy testing

```{r message = FALSE, warning = FALSE}
set.seed(100)

# Using best fit model from above with testing data
pred.lasso.f = predict(lasso.m.1, testing)
pred.lasso.f.prob = predict(lasso.m.1, testing, type = "prob")

# Evaluating in testing data with confusion matrix
eval.results = confusionMatrix(pred.lasso.f, testing$behav, positive = "1")
print(eval.results)
```

Final accuracy testing shows that the accuracy of the LASSO model is 0.62.

# Part 3: Ethical considerations of data-driven analysis in social epidemiology

## Question 1

*Braithwaite et al*

Overall, I do not think this paper is helpful in predicting early detection of suicidality. The paper that I've selected to analyze is the Braithwaite et al. paper predicting suicide from Twitter data. One thing that immediately stood out to me pertaining to the quality of analysis is the data quality. The authors describe recruiting through MTurk, which is Amazon's form of task deployment, where workers can sign up to do tasks for a negligible amount of money. As described in the paper, participants were paid the "market rate", which was a shocking 30-50 cents. Immediately, this should be cause for concern; people willing to participate in a study for a mere 30 cents are likely not representative of the general population. The study population could include a higher proportion of people struggling to make ends meet, and thus need an additional source of income, which could impact mental health and suicidality as a result of financial instability.

The authors fail to report the average age of the participants, which would be important when understanding suicide, particularly as age is a risk factor for suicide. Additionally, the authors don't present information on sex. Men are at higher risk of suicide than women, and this information would have been important to present as well for understanding potential bias.

Additionally, the authors explain that they carried out study recruitment at two different time periods, as initial recruitment did not result in a sufficient number of participants. This is important given that one period of recruitment took place in the summer and the second took place in the fall. If participants are subject to seasonal depression, then higher rates of suicidal tweets may have been observed in the second set of participants. Unfortunately, the authors do not have the statistical power to stratify by recruitment period. Further, the authors excluded 77 participants without explanation for why these participants were excluded.

They used a decision tree algorithm for feature selection and variable importance. When considering the quality of the analysis, the authors failed to describe how they split their data and whether they split 70/30. They also failed to describe whether and how they varied their hyperparameters, which would have been "cp" in this algorithm. They do mention that they used cross-validation, but don't mention what they set their cross-validation parameter as. Given the unbalanced nature of the data, they likely should have accounted for this; however, they don't describe whether they took this into account or not. 

## Question 2

The authors conclude that tweets including "achieve" and those including "religion" are protective against suicidality. This could be challenging for interventions that are attempting to identify text that is predictive of suicidality. If this algorithm is implemented in real time, it could be harmful if users are flagged for suicidal behavior without actually exhibiting suicidal behavior. For example, were users to tweet about topics other than those trained on, they might be flagged. This could lead to self-fulfilling behavior, in that being told they are at risk of suicide actually places them at higher risk of suicide.

Further, there exist disparities in internet access across the United States. Counties or neighborhoods could be identified as having users with high rates of suicide, or high rates of suicidal behavior, and resources could be funneled towards these neighborhoods. However, counties with lower internet access and fewer users may not be allocated these resources because they were not identified as having users with high suicide rates.

## Question 3













