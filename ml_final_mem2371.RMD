---
title: "Machine Learning Final"
author: "Megan Marziali"
date: "3/25/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(caret)
library(rpart.plot)
library(Amelia)
library(arsenal)
library(factoextra)
library(cluster)

set.seed(100)
```

# Part 1: Dietary patterns using unsupervised analysis

## Question 1: Construct a research question

The goal of this unsupervised analysis will be to develop classes of dietary patterns among women during pregnancy. These classes will be used as an exposure in future analyses, aiming to predict whether dietary patterns during pregnancy are predictive of postpartum mental health systems (such as post-partum depression). This is a predictive research question.

## Question 2: Running appropriate unsupervised analysis

```{r}
diet.data = 
  read.csv("./data/diet_data.csv", header = TRUE) %>% 
  select(
    "h_cereal_preg_Ter",
    "h_dairy_preg_Ter",
    "h_fastfood_preg_Ter",
    "h_fish_preg_Ter",
    "h_fruit_preg_Ter",
    "h_legume_preg_Ter",
    "h_meat_preg_Ter",
    "h_veg_preg_Ter",
  )

# Scaling data
colMeans(diet.data, na.rm = TRUE)
apply(diet.data, 2, sd, na.rm = TRUE)
```

I restricted the data to variables which assessed fdietary patterns during pregnancy. The total dataset is comprised of 1301 observations, and 8 variables.

```{r}
missmap(diet.data, main = "Missing values vs observed")
```

I used missmap to determine if there are any missing observations, which there are not. To determine clusters within the data, a hierarchical cluster analysis with the complete linkage method will be carried out.

```{r}
set.seed(100)

# Create Dissimilarity matrix
diss.matrix = dist(diet.data, method = "euclidean")

#Identifying the optimal number of clusters given complete linkage method
gap_stat_c = clusGap(diet.data, FUN = hcut, hc_method = "complete", K.max = 10, B = 50)
fviz_gap_stat(gap_stat_c)

#Characterizing the clusters
clusters.c = hcut(diet.data, k = 2, hc_func = "hclust", hc_method = "complete", hc_metric = "euclidian")

clusters.c$size
fviz_dend(clusters.c, rect = TRUE)
fviz_cluster(clusters.c)
```

The optimal number of clusters within this data is *2* clusters. There are 840 observations in the first cluster, and 461 observations in the second cluster.

```{r}
input.feature.vals = cbind(diet.data, cluster = clusters.c$cluster)
input.feature.vals %>%
 group_by(cluster) %>%
 summarise_all(mean) %>% 
  knitr::kable()
```

The first cluster has less cereal consumed during pregnancy by mothers than the second cluster; respondents also consumed slightly more dairy, fast food, and fish. Those in the second cluster consumed more cereal and slightly more fruit and meats. Both clusters consumed approximately the same amount of vegetables.

# Part 2: Choose your own supervised adventure

## Research question

The goal of this analysis is to generate hypotheses regarding social capital and toxic metals during pregnancy on child externalizing and internalizing behaviors. This hypotheses generating analysis will guide future research on the role of social capital on child behaviors, and whether interactions between social capital and toxic metals are salient for child behaviors. This is being done to generate hypotheses regarding the importance of both the built environment and the social environment on child well-being, and to gain a better understanding of how these factors interact in relation to child health outcomes.

## Loading and preparing data

Loading data into single data frame.

```{r dataprep, message = FALSE, warning = FALSE}
#Load data using path of where file is stored
load("./data/exposome.RData")

#Merge all data frames into a single data frame. FYI, this is just a shortcut by combining baseR with piping from tidyverse. There are other ways of merging across three data frames that are likely more elegant.

studydata = 
  merge(exposome,phenotype,by = "ID") %>% 
  merge(covariates, by = "ID")

#Strip off ID Variable
studydata$ID = NULL
```

Data cleaning and selecting relevant features to research question.

```{r message = FALSE, warning = FALSE}
studydata  = studydata %>% 
  select(hs_as_m_Log2,
         hs_cd_m_Log2,
         hs_co_m_Log2,
         hs_cs_m_Log2,
         hs_cu_m_Log2,
         hs_hg_m_Log2,
         hs_mn_m_Log2,
         hs_mo_m_Log2,
         hs_pb_m_Log2,
         hs_tl_mdich_None,
         hs_dde_madj_Log2,
         hs_ddt_madj_Log2,
         hs_hcb_madj_Log2,
         hs_pcb118_madj_Log2,
         hs_pcb138_madj_Log2,
         hs_pcb153_madj_Log2,
         hs_pcb170_madj_Log2,
         hs_pcb180_madj_Log2,
         hs_sumPCBs5_madj_Log2,
         hs_dep_madj_Log2,
         hs_detp_madj_Log2,
         hs_dmp_madj_Log2,
         hs_dmtp_madj_Log2,
         hs_pbde153_madj_Log2,
         hs_pbde47_madj_Log2,
         hs_pfhxs_m_Log2,
         hs_pfna_m_Log2,
         hs_pfoa_m_Log2,
         hs_pfos_m_Log2,
         hs_pfunda_m_Log2,
         hs_bpa_madj_Log2,
         hs_bupa_madj_Log2,
         hs_etpa_madj_Log2,
         hs_mepa_madj_Log2,
         hs_oxbe_madj_Log2,
         hs_prpa_madj_Log2,
         hs_trcs_madj_Log2,
         hs_mbzp_madj_Log2,
         hs_mecpp_madj_Log2,
         hs_mehhp_madj_Log2,
         hs_mehp_madj_Log2,
         hs_meohp_madj_Log2,
         hs_mep_madj_Log2,
         hs_mibp_madj_Log2,
         hs_mnbp_madj_Log2,
         hs_ohminp_madj_Log2,
         hs_oxominp_madj_Log2,
         hs_sumDEHP_madj_Log2,
         hs_cotinine_mcat_None,
         FAS_cat_None,
         hs_contactfam_3cat_num_None,
         hs_hm_pers_None,
         hs_participation_3cat_None,
         hs_Gen_Tot
         ) 

data.rec = studydata %>% 
  mutate(
    behav = cut(hs_Gen_Tot, 
                breaks = c(-Inf, 60, Inf),
                labels = c("0", "1"))
  ) %>% 
  select(-hs_Gen_Tot) %>% 
  select(everything(), behav)
```

Data exploration.

```{r message = FALSE, warning = FALSE, results = "asis"}
# Investigating missing data
missmap(studydata)

# No missingness observed.

# Exploring continuous/categorical variables of interest
table.1 = tableby(~ behav + FAS_cat_None + hs_contactfam_3cat_num_None +
                    hs_sumDEHP_madj_Log2 + hs_ohminp_madj_Log2 + hs_mibp_madj_Log2 + hs_mehp_madj_Log2,
                  data = data.rec,
        numeric.stats = c("mean","median", "range"))
summary(table.1, text = TRUE)
```

The data is unbalanced, which will be taken into account by down-sampling.

Partitioning data.

```{r message = FALSE, warning = FALSE}
set.seed(100)

#Partition data for use in demonstration
train.indices = createDataPartition(y = data.rec$behav, p = 0.7,list = FALSE)
training = data.rec[train.indices, ]
testing = data.rec[-train.indices, ]
```

## First Algorithm: Using LASSO for Feature Selection

Hyperparameter tuning and initial accuracy testing.

```{r, warning = FALSE, message = FALSE}
set.seed(100)

#Create grid to search lambda
lambda = 10^seq(-3,3, length = 100)

lasso.m1 = train(
  behav ~., 
  data = training, 
  method = "glmnet", 
  trControl = trainControl("cv", number = 10, sampling = "down"), 
  tuneGrid = expand.grid(alpha = 0, lambda = lambda)
)

#Print the values of alpha and lambda that gave best prediction
lasso.m1$bestTune

#Print all of the options examined
lasso.m1$results

# Model coefficients
coef(lasso.m1$finalModel, lasso.m1$bestTune$lambda)
varImp(lasso.m1)

# Make predictions
pred.lasso.1 = predict(lasso.m1, training)
pred.lasso.prob.1 = predict(lasso.m1, training, type = "prob")

# Model prediction performance
eval.results = confusionMatrix(pred.lasso.1, training$behav, positive = "1")
print(eval.results)

#Accuracy of this model is 0.65
```

## Second Algorithm: Elastic Net

```{r}
set.seed(100)

alpha = seq(0,1, length = 100)

en.m = train(
  behav ~., 
  data = training, 
  method = "glmnet", 
  trControl = trainControl("cv", number = 10, sampling = "down"), 
  tuneGrid = expand.grid(alpha = alpha, lambda = lambda)
)

#Print the values of alpha and lambda that gave best prediction
en.m$bestTune

#Print all of the options examined
en.m$results

# Model coefficients
coef(en.m$finalModel, en.m$bestTune$lambda)
varImp(en.m)

# Make predictions
pred.en = predict(en.m, training)
pred.en.prob = predict(en.m, training, type = "prob")

# Model prediction performance
eval.results = confusionMatrix(pred.en, training$behav, positive = "1")
print(eval.results)

#Accuracy of this model is 0.58
```

Given that the LASSO model produced slightly better accuracy than the elastic net model, the LASSO model will be used for final accuracy testing.

### Final accuracy testing

```{r message = FALSE, warning = FALSE}
set.seed(100)

# Using best fit model from above with testing data
pred.lasso.f = predict(lasso.m.1, testing)
pred.lasso.f.prob = predict(lasso.m.1, testing, type = "prob")

# Evaluating in testing data with confusion matrix
eval.results = confusionMatrix(pred.lasso.f, testing$behav, positive = "1")
print(eval.results)
```

Final accuracy testing shows that the accuracy of the LASSO model is 0.62.